\section{Methods for Concept Profiling}

%Here we give a high-level description of the overall method which consists of two steps: 1) extraction of concepts and 2) further extraction of elemental concepts and composite concepts.
In this section we describe our methods for building concept profiles. The process involves two steps: first, given a learning module's text data we need to extract the salient phrases that represent concepts. Subsection \ref{extraction} explores one way of automatically extracting concept phrases from a module's text data when no clear list of concepts is provided such as the case for MOOC courses. The second step in the process is to rank the extracted concepts according to their Elementa/Composite score (hence forth referred to as EC Score).

\subsection{Concept Extraction}\label{extraction}

%Here, we describe our method for concept extraction. Can the neural network be applied here? 

%We could say that we just use existing methods for this step, but if there's anything interesting to say here, e.g., using NN, or leveraging book index as training data, we should say that. 

As we have stated in the Related Work section, the problem of extracting concept phrases from text is not a new one in and of itself. Several semi-supervised and unsupervised methods have been proposed. Autophrase \cite{} and TopMine \cite{} are two notable unsupervised methods for salient phrase mining. In our work, we take a slightly different approach using neural networks. The resulting extracted concepts using our neural network are demonstrably better quality with higher precision and completeness than the results from TopMine or Autophrase. %might need to back this up with results in the eval section

Since our work pertains specifically to educational applications, we leverage on type of unique text data ubiquitiously available for the vast majority of learning modules: textbooks. Whether a learning module is taught in a traditional class setting or offered to independent learners through MOOCs, the subjects of the vast majority of learning modules have probably been covered by some textbook. What makes textbooks particularly useful to us as opposed to other types of educational text data is that each textbook has an expertly curated, clean, and complete list of concept phrases covered in the textbook; namely the textbook's index. Using the text data and index list of phrases from a textbook, we train a neural network to automatically identify and label concept phrases in other types of educational text data which cover the same subject, such as MOOC lecture transcripts, and which do not have a predefined index. In section \ref{experiment_setup} we describe such a setup using two textbooks and four MOOCs which cover the same topics. In the rest of this section, we describe the design of the auto-indexing neural network we used.

%add info about stemming and preprocessing
The concept phrase extraction task is given a text document, and asked to determine which words and phrases in the text represent concepts and which do not. For this task we use the raw text from our textbook data sets as documents, and use the index from each textbook as labels. This is quite similar to the NP Chunking problem, wherein we must determine which parts of the text are Noun Phrases. We use a basic IOB (Inside, Outside, Beginning) scheme for our chunking.
Our neural network is designed to produce a ``B'' tag for the first word in a keyphrase in the textbook's index and then an ``I'' tag for every word following the first word in the same keyphrase. If a word is not part of any keyphrase, then it is tagged with an ``O''.

We used a fairly simple LSTM network to perform the tagging. We split each dataset into 75\% train and 25\% test. We trained each model over 8 epochs. We used a word embedding dimension and a hidden state dimension of 32. We use a single LSTM layer and then an addition linear layer which maps the hidden state of the LSTM to the tag space. We then perform a softmax over our tags. Our prediction is the max over our different tags.
%add figure of NN setup


\subsection{Building the Concept Profile}

%Here we describe the proposed (new) methods for discovering the two kinds of concepts. 

%Since this is a new challenge, we should devote most of our effort here to improve the methods. We may also propose multiple strategies if possible. 

The intuition behind our method for ranking concepts according to their EC Score is the observation that in a learning module, new concepts are introduced using other more elemental concepts. For example, in Caclulus, the concept "derivative" can be introduced to students using concepts such as "function", "slope", and "tangent". Later on in Multivariabel Calculus, "differential" and "derivative" can be used to introduce the concept of "partial derivative" to students. In general, a learning module's Concept Profile follows a roughly hierarchical structure with more Elemental concepts being used to explain or introduce more Composite concepts on the EC Score scale. Figure \ref{concept_space} illustrates this roughly hierarchical layered clusters structure to the learning module's Concept Profile. Intuitively, we expect that the more Elemental a concept is, the more clusters it is a part of, whereas the more Composite a concept is, the less clusters it appears in. This makes sense because more Elemental concepts can be used to explain a wider range of concepts that more "specialized" Composite concepts. Using the above intuition as our working hypothesis, we design a roughly hierarchical clustering method called "Layered Fuzzy C-Means" to detect these layered concept clusters.
%add concept space figure

\subsubsection{Concept Embeddings}
An important part of any text clustering algorithm is getting good embedding vector representation for the data points. This step is especially important for us since we need concept phrase embeddings to accurately capture the context of each concept so that concepts that appear in close locality to each other and carry similar contexts may successfully be clustered together. Since this is very dependent on the learning module we are building the Concept Profile for, pre-trained word embeddings such as those from BERT \cite{} or pre-trained GloVe \cite{}, are not compatible with our application. Instead, we must use concept phrase embeddings that have been trained on the learning module text data itself.

Since we have already trained a bidirectional LSTM to detect concept keywords and phrases in the previous step, we can easily grab the "tagged" concepts' embeddings from the final hidden layer of the LSTM after training. This will give us for each concept phrase one 32 dimensional "forward" embedding vector and one 32 dimensional "backward" embedding vector. To capture the complete context of the concept phrase from both directions, we concatenated the two vectors into one 64 dimensional "bidirectional" embedding vector for each detected concept. We use this 64 dimensional vector in the following clustering step.

We found that using the bidirectional embedding vectors from our trained LSTM yields higher quality concept clusters than other commonly used word embedding methods such as Word2Vec \cite{}. It also does not present us the problem of how to combine individual word embeddings into multi-word concept phrase embeddings; and since different concepts might consist of different length phrases, simply concatenating word embeddings to form phrase embeddings is not feasable. Further work on obtaining high quality concept phrase embeddings is left as a future work direction.

\subsubsection{Layered Fuzzy C-Means}
Our Layered Fuzzy C-Means clustering algorithm follows a loosely hierarchical clustering scheme where each iteration or "layer" consists of a run of the Fuzzy C-Means algorithm. The Fuzzy C-Means algorith was first introduced by ... in 198.. \cite{} and later improved by .. \cite{}. It is the standard algorithm for fuzzy clustering; the counterpart to the well known K-Means \cite{}. Fuzzy clustering expresses cluster membership as a probability distribution thus allowing data points to potentially belong to multiple "overlapping" clusters if their cluster membership probability passes a certain threshold. This is important for our purpose since our working hypothesis is that a more Elemental concept would be used to explain other Composite concepts, thus it would make sense for the more Elemental concept to belong to multiple clusters representing the Composite concepts it helps explain. See Figure \ref{concept_space}.

The input for the Layered Fuzzy C-Means is: 1) $X$: a 64 by N matrix which holds the 64 dimensional concept embeddings of the learning module's N extracted concepts from the Concept Extraction step \ref{extraction}, we will refer to this as the data point matrix, 2) the parameter $INIT\_CLUSTERS$ which is a function of $N$ which specifies the number of clusters in the initial layer, 3) and the parameters $min_p$ and $min_q$ which specify the probability threshold for cluster membership and the clustering quality threshold for retaining layers.

The Layered Fuzzy C-Means outputs: 1) a dictionary of layer cluster membership matrices:  $U = \{i: U_{i} | for\ each\ layer\ i\}$ where each $U_{i}$ is a $C_{i}$ by $N$ matrix where $C_{i}$ is the number of clusters in layer $i$, 2) a dictionary of layer quality indices: $Q = \{i: q_{i} | for\ each\ layer\ i\}$.

The first iteration of Layered Fuzzy C-Means runs the $skfuzzy$'s python library implementation of the Fuzzy C-Means algorithm $fcm()$ \cite{} on the data matrix $X$ with number of clusters initialized to $INIT\_CLUSTERS$. The resulting membership probability matrix is saved to the dictionary $U$. For each subsequent iteration, the number of clusters $INIT\_CLUSTERS$ is decremented by 1; the final iteration runs $fcm()$ with number of clusters equal to 1. Figure \ref{layered_fcm} details our implementation of the Layered Fuzzy C-Means.
% add algorithm figure

\subsubsection{Postprocessing for Hard Cluster Membership}
Once our Layered Fuzzy C-Means algorithm outputs the dictionary of layer cluster membership probability matrices $U$, we need to perform some postprocessing to define the hard borders of each cluster in each layer. This will convert each $U_{i}$ in the dictionary from a probability matrix to a matrix $H_{i}$ of 1's and 0's, where an entry of 1 at position $(k, j)$ indicates that the $j$'th data point belongs to the $k$'th cluster in the layer, whereas an entry of 0 indicates that the data point does not belong to the cluster. This postprocessing step is needed both for calculating the clustering quality in \ref{pruning_layers} and for calculating the EC Score in \ref{calc_ec}.

Let $u_{k,j}$ be the probability entry at position $(k,j)$ of the layer matrix $U_{i}$. We calculate the entry $h_{k,j}$ of the matrix $H_{i}$ using the formula:

\begin{equation}
h_{k,j} = 
\begin{cases}
    1,& \text{if } u_{k,j}\geq max_{k,j}(U_{i}) * min_p\\
    0,              & \text{otherwise}
\end{cases}
\end{equation}

where $max_{k,j}(U_{i})$ is the value of the largest membership probability entry in $U_{i}$ and $min_p \in [0,1]$ is a user defined input to the Layered Fuzzy C-Means.

One important thing to notice here is that in each layer, there can be data points (concepts) that belong to more than one cluster (the sum of their column in $H_{i}$ is greater than 1), as well as data points that belong to no cluster (the sum of their column in $H_{i}$ is equal to 0).

\subsubsection{Pruning for Quality Layers}\label{pruning_layers}
Once we calculate the $H_{i}$ matrix for each layer $i$, we still need to prune the layers to retain only layers that contain good quality clustering. This is an important step in addition to the previous postprocessing step because in the previous step, even though we restrict cluster membership to probabilities that pass the minimum probability threshold, this minimum probability threshold is dependent on the probability distribution of $U_{i}$ and thus does not guarantee us a layer with good quality clusters. To understand this more clearly, suppose for a layer $i$, $max_{k,j}(U_{i}) \approx 1/N$ meaning that for this layer, membership probabilities are more or less uniformly distributed. We will end up with a $H_{i}$ matrix of mostly 1's, which is a very bad quality clustering. Thus it is still important to prune such layers out.

To determine whether or not to retain a layer, we use the Calinski Harabaz Index (CH Index)and the user defined $min_q$ threshold. The CH Index is calculated as:

\begin{equation}
HC-Index_{i} = \frac{B_{i}(k)}{W_{i}(k)}\frac{N-k}{k-1}
\end{equation}

where $B_{i}(k)$ measures the between cluster variance of the $k$ clusters in layer $i$, and $W_{i}(k)$ is the sum of the within cluster variances of the $k$ clusters in layer $i$. The intuition behind the HC Index is that layers with higher HC Index contain tighter and well separated clusters.
We calculate $HC-Index_{i}$ for each layer $i$ and store it in the layer quality index dictionary $Q$ as $q_{i}$. Finally, we only retain layers with $q_{i} \geq max_{i}(q_{i}) * min_q$.

\subsubsection{Computing the EC Score}\label{calc_ec}

